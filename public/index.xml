<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Cornell RPAL</title>
    <link>https://rpal.cs.cornell.edu/index.xml</link>
    <description>Recent content on Cornell RPAL</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 07 Feb 2017 05:56:51 -0500</lastBuildDate>
    <atom:link href="https://rpal.cs.cornell.edu/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Zach Zweig Vinegar</title>
      <link>https://rpal.cs.cornell.edu/people/zach/</link>
      <pubDate>Tue, 07 Feb 2017 05:56:51 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/people/zach/</guid>
      <description>&lt;p&gt;I am a senior computer science major in the the College of Engineering at Cornell University. My current research interests lie in the areas of human-robot interaction, computer vision, and augmented reality. I am also a member of Cornell&amp;rsquo;s Robotic Personal Assistants Lab run by Prof. Ross Knepper. Currently, I am working on a telepresence robot equipped with a 360 degree RGB-D sensor. The goal is to use the data coming from the sensor to enable the robot to seamlessly and smoothly navigate crowded pedestrian environments. More details about me can be found on my website.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Valts Blukis</title>
      <link>https://rpal.cs.cornell.edu/people/valts/</link>
      <pubDate>Mon, 06 Feb 2017 09:58:51 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/people/valts/</guid>
      <description>&lt;p&gt;Valts is a first-year PhD student in computer science. He graduated with a degree in Electrical and Electronic Enginnering from Nanyang Technological University in Singapore. In the past he has worked on rescue robotics and stereo vision. He is interested in robotics and machine learning, in particular developing algorithms that would improve robot understanding of the real world and allow competent interactions with people.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Social Navigation</title>
      <link>https://rpal.cs.cornell.edu/projects/social_nav/</link>
      <pubDate>Wed, 25 Jan 2017 20:09:33 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/projects/social_nav/</guid>
      <description>&lt;p&gt;Some other content&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Unfamiliar Gestures</title>
      <link>https://rpal.cs.cornell.edu/projects/gestures/</link>
      <pubDate>Wed, 25 Jan 2017 20:09:23 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/projects/gestures/</guid>
      <description>&lt;p&gt;Human communication is highly multimodal, including speech, gesture, gaze, facial
expressions, and body language. Robots serving as human teammates must act on such
multimodal communicative inputs from humans, even when the message may not be
clear from any single modality. In this paper, we explore a method for achieving increased
understanding of complex, situated communications by leveraging coordinated
natural language, gesture, and context.&lt;/p&gt;

&lt;p&gt;Our work departs from the traditional model of gesture recognition in that the set of gestures it
can recognize is not limited to the gestural lexicon used for its training. Even in simplified
domains, naive classifiers can fail to recognize instances of trained gestures due to human
gestural variability. Humans resort to gesture when speech is insufficient, such as due to
inability to recall a word, inability to be heard, or inadequate time to formulate speech.
For these reasons, gesture is prevalent in human discourse. Yet gestures defy attempts
at canonical classification both due to variations within and among individuals and due
to their subjective interpretations. We define the unfamiliar gesture understanding
problem: given an observation of a previously unseen gesture (i.e. a gesture of a class
not present in any training data given to the system), we wish to output a contextually
reasonable description in natural language of the gesture’s intended meaning.&lt;/p&gt;

&lt;p&gt;This problem is an instance of the machine learning problem of zero-shot learning,
a burgeoning area of machine learning that seeks to classify data without having seen
examples of its class in the training stage. Most prior work in the area makes
use of a multimodal dataset to perform the zero-shot task. However, the zero-shot task
has not yet been demonstrated for gestural data. In the related one-shot learning task,
gesture understanding has been shown from only one example of a given class in the
training stage. The primary drawback of such approaches is their reliance on
a fixed lexicon of gestures. We remove this drawback by creating a novel multimodal
embedding space using techniques from convolutional neural nets to handle variable
length gestures and allow for the description of arbitrary unfamiliar gestural data.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Solar Airship</title>
      <link>https://rpal.cs.cornell.edu/projects/blimp/</link>
      <pubDate>Wed, 25 Jan 2017 20:09:13 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/projects/blimp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Ross Knepper</title>
      <link>https://rpal.cs.cornell.edu/people/ross/</link>
      <pubDate>Wed, 25 Jan 2017 16:12:51 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/people/ross/</guid>
      <description>&lt;p&gt;Ross A. Knepper is an Assistant Professor in the Department of Computer Science at Cornell University. His research focuses on the theory, algorithms, and mechanisms of automated assembly and human-robot collaboration. Previously, Ross was a Research Scientist in the Distributed Robotics Lab at MIT. Ross received his M.S and Ph.D. degrees in Robotics from Carnegie Mellon University in 2007 and 2011. Before his graduate education, Ross worked in industry at Compaq, where he designed high-performance algorithms for scalable multiprocessor systems; and also in commercialization at the National Robotics Engineering Center, where he adapted robotics technologies for customers in government and industry. Ross has served as a volunteer for Interpretation at Death Valley National Park, California.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://rpal.cs.cornell.edu/about/</link>
      <pubDate>Wed, 25 Jan 2017 00:25:59 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/about/</guid>
      <description>&lt;p&gt;The Robotic Personal Assistants Lab is focused on advancing the state of the art in human-robot interaction,
robotic manipulation, automated assembly, and multi-agent planning and coordination. We work on projects fusing
theoretical advances with empirically assessed implementations.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Contact Information</title>
      <link>https://rpal.cs.cornell.edu/contact/</link>
      <pubDate>Wed, 25 Jan 2017 00:25:51 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/contact/</guid>
      <description>&lt;p&gt;Contact us via email at rpal@cs.{our university}.edu&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Paper Accepted to HRI 2017</title>
      <link>https://rpal.cs.cornell.edu/news/hri2017/</link>
      <pubDate>Tue, 24 Jan 2017 23:47:43 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/news/hri2017/</guid>
      <description>&lt;p&gt;The paper &amp;ldquo;Implicit Communication in a Joint Action&amp;rdquo;, by &lt;a href=&#34;https://rpal.cs.cornell.edu/people/ross/&#34;&gt;Ross Knepper&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Daryl Sew</title>
      <link>https://rpal.cs.cornell.edu/people/daryl/</link>
      <pubDate>Wed, 26 Oct 2016 21:19:59 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/people/daryl/</guid>
      <description>&lt;p&gt;Computer Science undergraduate interested in computer vision, machine learning and robotics.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Vitchyr Pong</title>
      <link>https://rpal.cs.cornell.edu/people/vitchyr/</link>
      <pubDate>Tue, 26 Jan 2016 21:20:10 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/people/vitchyr/</guid>
      <description>&lt;p&gt;Undergraduate in ECE and CS, moved on to a PhD at UC Berkeley.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Alex Volkov</title>
      <link>https://rpal.cs.cornell.edu/people/alex/</link>
      <pubDate>Tue, 26 Jan 2016 21:19:52 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/people/alex/</guid>
      <description>&lt;p&gt;ECE undergraduate, moved on to a Master&amp;rsquo;s at the CMU Robotics Institute.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Wil Thomason</title>
      <link>https://rpal.cs.cornell.edu/people/wil/</link>
      <pubDate>Sun, 25 Jan 2015 16:12:51 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/people/wil/</guid>
      <description>&lt;p&gt;Wil is a second-year PhD student in computer science. He is interested in a broad range of topics in
robotics and CS, including multi-agent planning, human-robot interaction (with a particular focus on
language and gesture understanding), and machine learning for robotics.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Minae Kwon</title>
      <link>https://rpal.cs.cornell.edu/people/minae/</link>
      <pubDate>Sun, 26 Jan 2014 21:20:05 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/people/minae/</guid>
      <description>&lt;p&gt;I’m interested in facilitating collaboration between robots and humans by computationally expressing
findings from social and cognitive psychology to model human behavior and appropriately design robot
behavior. Currently, I am working on how humans form expectations of social robots and how we can
help people form more accurate mental models of them.  When not doing research, I enjoy debating
(competitively), watching movies, and traveling.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Christoforos Mavrogiannis</title>
      <link>https://rpal.cs.cornell.edu/people/chris/</link>
      <pubDate>Sat, 25 Jan 2014 15:50:27 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/people/chris/</guid>
      <description>&lt;p&gt;Chris received his Diploma degree (equivalent to MSc) in Mechanical Engineering from the National
Technical University of Athens, Greece in 2013. He is currently pursuing a PhD in Mechanical
engineering, working on the development of motion planning algorithms for seamless navigation of
pedestrian environments. In the past he has done research on robotic grasping and robotic hand
design.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>