<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Cornell RPAL</title>
    <link>https://rpal.cs.cornell.edu/index.xml</link>
    <description>Recent content on Cornell RPAL</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 07 Feb 2017 05:56:51 -0500</lastBuildDate>
    <atom:link href="https://rpal.cs.cornell.edu/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Zach Zweig Vinegar</title>
      <link>https://rpal.cs.cornell.edu/people/zach/</link>
      <pubDate>Tue, 07 Feb 2017 05:56:51 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/people/zach/</guid>
      <description>&lt;p&gt;I am a senior computer science major in the College of Engineering at Cornell University. My current research interests lie in the areas of human-robot interaction, computer vision, and augmented reality. I am also a member of Cornell&amp;rsquo;s Robotic Personal Assistants Lab run by Prof. Ross Knepper. Currently, I am working on a telepresence robot equipped with a 360 degree RGB-D sensor. The goal is to use the data coming from the sensor to enable the robot to seamlessly and smoothly navigate crowded pedestrian environments. More details about me can be found on my website.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Valts Blukis</title>
      <link>https://rpal.cs.cornell.edu/people/valts/</link>
      <pubDate>Mon, 06 Feb 2017 09:58:51 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/people/valts/</guid>
      <description>&lt;p&gt;Valts is a first-year PhD student in computer science. He graduated with a degree in Electrical and
Electronic Engineering from Nanyang Technological University in Singapore. In the past he has worked
on rescue robotics and stereo vision. He is interested in robotics and machine learning, in
particular developing algorithms that would improve robot understanding of the real world and allow
competent interactions with people.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Social Navigation</title>
      <link>https://rpal.cs.cornell.edu/projects/social_nav/</link>
      <pubDate>Wed, 25 Jan 2017 20:09:33 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/projects/social_nav/</guid>
      <description>&lt;p&gt;Despite the great progress in the field of robotic navigation over the past few decades, navigating a human environment remains a hard task for a robot, due to the lack of formal rules guiding traffic, the lack of explicit communication among agents and the unpredictability of human behavior. Existing approaches often result in robot motion that is hard to read, which causes unpredictable human reactions to which the robot in turn reacts to, contributing to an oscillatory joint behavior that hinders humans’ paths. We argue that the root of the problem lies in the failure from the robot’s part to convey consistently its intentions to human observers.&lt;/p&gt;

&lt;p&gt;This project aims at developing an autonomous robotic system, capable of navigating crowded environments in a socially competent fashion. To this end, we develop novel models, algorithms, software and systems, which we plan on validating experimentally in real-world scenarios.&lt;/p&gt;

&lt;p&gt;The main novelty of our approach lies in the development of a novel planning framework for closed-loop navigation in dynamic multi-agent workspaces. The core of the framework is a novel topological representation, based on braid groups, that models the collective behavior of multiple agents. Based on this representation and employing data-driven techniques, our algorithms generate motion plans that are consistent with the perceived context, thus resulting in socially competent robot behaviors that allow for smooth integration of mobile robots in crowded human environments. Our framework is inspired by insights from studies on pedestrian behavior and action interpretation and leverages the power of implicit communication to overcome the complications of the uncertainties induced by the imperfections of existing models of human decision making.&lt;/p&gt;

&lt;p&gt;Our robot platform is a Suitable Technologies Beam Pro, equipped with on-board computation and an Occam Omni Stereo 360 RGBD camera, interfaced through ROS.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Unfamiliar Gestures</title>
      <link>https://rpal.cs.cornell.edu/projects/gestures/</link>
      <pubDate>Wed, 25 Jan 2017 20:09:23 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/projects/gestures/</guid>
      <description>&lt;p&gt;Human communication is highly multimodal, including speech, gesture, gaze, facial
expressions, and body language. Robots serving as human teammates must act on such
multimodal communicative inputs from humans, even when the message may not be
clear from any single modality. In this paper, we explore a method for achieving increased
understanding of complex, situated communications by leveraging coordinated
natural language, gesture, and context.&lt;/p&gt;

&lt;p&gt;Our work departs from the traditional model of gesture recognition in that the set of gestures it
can recognize is not limited to the gestural lexicon used for its training. Even in simplified
domains, naive classifiers can fail to recognize instances of trained gestures due to human
gestural variability. Humans resort to gesture when speech is insufficient, such as due to
inability to recall a word, inability to be heard, or inadequate time to formulate speech.
For these reasons, gesture is prevalent in human discourse. Yet gestures defy attempts
at canonical classification both due to variations within and among individuals and due
to their subjective interpretations. We define the unfamiliar gesture understanding
problem: given an observation of a previously unseen gesture (i.e. a gesture of a class
not present in any training data given to the system), we wish to output a contextually
reasonable description in natural language of the gesture’s intended meaning.&lt;/p&gt;

&lt;p&gt;This problem is an instance of the machine learning problem of zero-shot learning,
a burgeoning area of machine learning that seeks to classify data without having seen
examples of its class in the training stage. Most prior work in the area makes
use of a multimodal dataset to perform the zero-shot task. However, the zero-shot task
has not yet been demonstrated for gestural data. In the related one-shot learning task,
gesture understanding has been shown from only one example of a given class in the
training stage. The primary drawback of such approaches is their reliance on
a fixed lexicon of gestures. We remove this drawback by creating a novel multimodal
embedding space using techniques from convolutional neural nets to handle variable
length gestures and allow for the description of arbitrary unfamiliar gestural data.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Solar Airship</title>
      <link>https://rpal.cs.cornell.edu/projects/blimp/</link>
      <pubDate>Wed, 25 Jan 2017 20:09:13 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/projects/blimp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Ross Knepper</title>
      <link>https://rpal.cs.cornell.edu/people/ross/</link>
      <pubDate>Wed, 25 Jan 2017 16:12:51 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/people/ross/</guid>
      <description>&lt;p&gt;Ross A. Knepper is an Assistant Professor in the Department of Computer Science at Cornell University. His research focuses on the theory, algorithms, and mechanisms of automated assembly and human-robot collaboration. Previously, Ross was a Research Scientist in the Distributed Robotics Lab at MIT. Ross received his M.S and Ph.D. degrees in Robotics from Carnegie Mellon University in 2007 and 2011. Before his graduate education, Ross worked in industry at Compaq, where he designed high-performance algorithms for scalable multiprocessor systems; and also in commercialization at the National Robotics Engineering Center, where he adapted robotics technologies for customers in government and industry. Ross has served as a volunteer for Interpretation at Death Valley National Park, California.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://rpal.cs.cornell.edu/about/</link>
      <pubDate>Wed, 25 Jan 2017 00:25:59 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/about/</guid>
      <description>&lt;p&gt;The Robotic Personal Assistants Lab is focused on advancing the state of the art in human-robot interaction,
robotic manipulation, automated assembly, and multi-agent planning and coordination. We work on projects fusing
theoretical advances with empirically assessed implementations.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Contact Information</title>
      <link>https://rpal.cs.cornell.edu/contact/</link>
      <pubDate>Wed, 25 Jan 2017 00:25:51 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/contact/</guid>
      <description>&lt;div class=&#34;text-center&#34;&gt;Contact us via email at &lt;b&gt;rpal@cs.{our&amp;nbsp;university}.edu&lt;/b&gt;&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Paper Accepted to HRI 2017</title>
      <link>https://rpal.cs.cornell.edu/news/hri2017/</link>
      <pubDate>Tue, 24 Jan 2017 23:47:43 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/news/hri2017/</guid>
      <description>&lt;p&gt;The paper &amp;ldquo;Implicit Communication in a Joint Action&amp;rdquo;, by &lt;a href=&#34;https://rpal.cs.cornell.edu/people/ross/&#34;&gt;Ross Knepper&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Paper presented at WAFR 2016</title>
      <link>https://rpal.cs.cornell.edu/news/wafr2016/</link>
      <pubDate>Tue, 20 Dec 2016 23:47:43 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/news/wafr2016/</guid>
      <description>&lt;p&gt;The paper &amp;ldquo;Decentralized Multi-Agent Navigation Planning with Braids&amp;rdquo;, by &lt;a href=&#34;https://rpal.cs.cornell.edu/people/chris/&#34;&gt;Christoforos Mavrogiannis&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Daryl Sew</title>
      <link>https://rpal.cs.cornell.edu/people/daryl/</link>
      <pubDate>Wed, 26 Oct 2016 21:19:59 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/people/daryl/</guid>
      <description>&lt;p&gt;Computer Science undergraduate interested in computer vision, machine learning and robotics.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Vitchyr Pong</title>
      <link>https://rpal.cs.cornell.edu/people/vitchyr/</link>
      <pubDate>Tue, 26 Jan 2016 21:20:10 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/people/vitchyr/</guid>
      <description>&lt;p&gt;Undergraduate in ECE and CS, moved on to a PhD at UC Berkeley.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Alex Volkov</title>
      <link>https://rpal.cs.cornell.edu/people/alex/</link>
      <pubDate>Tue, 26 Jan 2016 21:19:52 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/people/alex/</guid>
      <description>&lt;p&gt;ECE undergraduate, moved on to a Master&amp;rsquo;s at the CMU Robotics Institute.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Wil Thomason</title>
      <link>https://rpal.cs.cornell.edu/people/wil/</link>
      <pubDate>Sun, 25 Jan 2015 16:12:51 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/people/wil/</guid>
      <description>&lt;p&gt;Wil is a second-year PhD student in computer science. He is interested in a broad range of topics in
robotics and CS, including multi-agent planning, human-robot interaction (with a particular focus on
language and gesture understanding), and machine learning for robotics.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Minae Kwon</title>
      <link>https://rpal.cs.cornell.edu/people/minae/</link>
      <pubDate>Sun, 26 Jan 2014 21:20:05 -0500</pubDate>
      
      <guid>https://rpal.cs.cornell.edu/people/minae/</guid>
      <description>&lt;p&gt;I’m interested in facilitating collaboration between robots and humans by computationally expressing
findings from social and cognitive psychology to model human behavior and appropriately design robot
behavior. Currently, I am working on how humans form expectations of social robots and how we can
help people form more accurate mental models of them.  When not doing research, I enjoy debating
(competitively), watching movies, and traveling.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>